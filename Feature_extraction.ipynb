{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1678619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_CREATION\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e0d1e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: speechbrain in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: hyperpyyaml in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (1.2.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (25.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (1.15.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.9 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (2.7.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (2.7.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (4.67.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from speechbrain) (0.32.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from torch>=1.9->speechbrain) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from torch>=1.9->speechbrain) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from torch>=1.9->speechbrain) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from torch>=1.9->speechbrain) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from torch>=1.9->speechbrain) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from torch>=1.9->speechbrain) (2025.5.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from torch>=1.9->speechbrain) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.9->speechbrain) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from huggingface_hub->speechbrain) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from huggingface_hub->speechbrain) (2.32.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from tqdm->speechbrain) (0.4.6)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from hyperpyyaml->speechbrain) (0.18.12)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from jinja2->torch>=1.9->speechbrain) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from requests->huggingface_hub->speechbrain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from requests->huggingface_hub->speechbrain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from requests->huggingface_hub->speechbrain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rishi s etagi\\desktop\\medivoice\\.venv\\lib\\site-packages (from requests->huggingface_hub->speechbrain) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install speechbrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ec4c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm import tqdm\n",
    "from speechbrain.pretrained import SpeakerRecognition # type:ignore \n",
    "import torchaudio\n",
    "# from pyAudioAnalysis import audioTrainTest as aT\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efe7b0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = r\"C:\\Users\\Rishi S Etagi\\Desktop\\medivoice\\LIBRI\"\n",
    "\n",
    "OUTPUT_CSV = \"speech_features.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd2c2344",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2703 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2703/2703 [01:38<00:00, 27.49it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict_gender(filepath):\n",
    "    import librosa\n",
    "    y, sr = librosa.load(filepath, sr=16000)\n",
    "    \n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch = pitches[magnitudes > np.median(magnitudes)]\n",
    "    pitch = pitch[pitch > 0]\n",
    "\n",
    "    if len(pitch) == 0:\n",
    "        return \"unknown\"\n",
    "    \n",
    "    pitch_mean = np.mean(pitch)\n",
    "    pitch_min = np.min(pitch)\n",
    "    pitch_max = np.max(pitch)\n",
    "    pitch_depth = pitch_max - pitch_min\n",
    "\n",
    "    energy = np.sum(librosa.feature.rms(y=y))\n",
    "\n",
    "    # Basic logic:\n",
    "    # Male generally has lower mean pitch and lower pitch depth\n",
    "    # Female generally has higher mean pitch and higher energy\n",
    "\n",
    "    if pitch_mean < 160 and pitch_depth < 40:\n",
    "        return 'male'\n",
    "    elif pitch_mean >= 160 and energy > 0.1:\n",
    "        return 'female'\n",
    "    else:\n",
    "        # ambiguous case, fallback to pitch_mean only\n",
    "        return 'female' if pitch_mean >= 150 else 'male'\n",
    "\n",
    "# def predict_gender(filepath):\n",
    "#     try:\n",
    "#         [Result, P, classNames] = aT.fileClassification(filepath, \"svm_gender_model\", \"svm\")\n",
    "#         return classNames[int(Result)]\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error detecting gender for {filepath}: {e}\")\n",
    "#         return \"unknown\"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def extract_features(filepath):\n",
    "    y, sr = librosa.load(filepath, sr=16000)\n",
    "\n",
    "    # MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfcc_mean = np.mean(mfccs, axis=1)\n",
    "\n",
    "    # Pitch-related features\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
    "    pitch = pitches[magnitudes > np.median(magnitudes)]\n",
    "    pitch = pitch[pitch > 0]  # Filter out zeros\n",
    "\n",
    "    pitch_mean = np.mean(pitch) if len(pitch) > 0 else 0\n",
    "    pitch_min = np.min(pitch) if len(pitch) > 0 else 0\n",
    "    pitch_max = np.max(pitch) if len(pitch) > 0 else 0\n",
    "    pitch_depth = pitch_max - pitch_min\n",
    "\n",
    "    # Energy\n",
    "    energy = np.sum(librosa.feature.rms(y=y))\n",
    "\n",
    "    # Speaking Rate\n",
    "    envelope = np.abs(y)\n",
    "    peaks, _ = find_peaks(envelope, height=0.02, distance=1000)\n",
    "    duration = len(y) / sr\n",
    "    speaking_rate = len(peaks) / duration if duration > 0 else 0\n",
    "\n",
    "    gender = predict_gender(filepath)\n",
    "\n",
    "    return {\n",
    "        \"filename\": os.path.basename(filepath),\n",
    "        \"pitch_mean\": pitch_mean,\n",
    "        \"pitch_min\": pitch_min,\n",
    "        \"pitch_max\": pitch_max,\n",
    "        \"pitch_depth\": pitch_depth,\n",
    "        \"energy\": energy,\n",
    "        \"speaking_rate\": speaking_rate,\n",
    "        \"gender\": gender,\n",
    "        **{f\"mfcc_{i+1}\": mfcc_mean[i] for i in range(len(mfcc_mean))}\n",
    "    }\n",
    "\n",
    "# Extraction loop\n",
    "features = []\n",
    "for root, dirs, files in os.walk(AUDIO_DIR):\n",
    "    for file in tqdm(files):\n",
    "        if file.endswith(\".flac\"):\n",
    "            path = os.path.join(root, file)\n",
    "            try:\n",
    "                f = extract_features(path)\n",
    "                features.append(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44b4c954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\n",
      "female    2703\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['gender'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32456138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(features)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
